---
title: "Descriptive Stats"
author: "Aaron"
date: "7/31/2018"
output: 
  html_document:
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
library(tidyverse)
library(here)
library(haven)
library(recipes)
library(pander)
library(skimr)
library(rpart)
library(rsample)
library(hmeasure)
library(rpart.plot)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::read_chunk(here("scripts", "classification", "01load.R"))
knitr::read_chunk(here("scripts", "classification","02preprocess.R"))
knitr::read_chunk(here("scripts", "classification","03rpart.R"))
```

# Descriptivs

```{r load}
```

```{r, results='asis'}
is_binary <- function(x, enforce_numeric = TRUE){
  if(enforce_numeric&(!is.numeric(x)))return(FALSE)
  else{
    length(unique(x))==2
  }
}

data %>%
  mutate_if(is_binary, as.factor) %>% 
  skim() %>% 
  pander()
```

The few missing values are for the following steps mean/mode imputed.

# Classification Tree

First we want to evaluate the out of sample performance of the model. For that propuse I utilze cross-validation. Due to the small N (hence compuation will be fast) I choose leave one out crossvalidation, to get the lowest possible bias. After that I treat the results as if they come from one model to make it possible to make utilze nonbinary performance measures. This should yield valid out of sample perfomance estimates.

```{r preprocess}
```

```{r rpart-loocv}
```

# Best Model According to F-Measure

```{r}
data_cv_metrics %>%
  filter(F == max(F)) %>% 
  ungroup() %>% 
  select(H, AUCH, Spec, Sens, TP, FP, TN, FN) %>% 
  .[1,] %>% 
  pander()
```

```{r rpart-full-f}
```

# Best Model According to H-Measure

```{r}
data_cv_metrics %>%
  filter(H == max(H)) %>% 
  ungroup() %>% 
  select(H, AUCH, Spec, Sens, TP, FP, TN, FN) %>% 
  .[1,] %>% 
  pander()
```

```{r rpart-full-h}
```
